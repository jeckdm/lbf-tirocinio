\documentclass[../../main.tex]{subfiles}

\graphicspath{{\subfix{../../immagini/}}}

\begin{document}
    Come introdotto nel precedente paragrafo, esistono diverse tecniche di model evaluation e model selection. In ognuna di queste tecniche vengono ovviamente utilizzate delle metriche per quantificare la bontà del modello su cui si sta lavorando.

    In ognuno dei nostri esperimenti sui modelli le metriche utilizzate sono le stesse, queste vengono brevemente introdotte di seguito, insieme ai relativi vantaggi e svantaggi.

    \paragraph{Accuratezza}
    L'accuratezza è una metrica che quantifica la bontà di un modello tramite il rapporto tra il numero di predizioni corrette ed il numero totale di predizioni. Nel contesto dei nostri esperimenti, in cui trattiamo un problema di classificazione binaria, questa può essere formalmente espressa come: 

    \begin{equation}
        \mathrm{Accuratezza} = \frac{VP + VN}{VP + VN + FP + FN},
    \end{equation}

    dove $VP$ e $VN$ rappresentano rispettivamente il numero di `veri positivi' e `veri negativi': esempi rispettivamente con etichetta positiva e negativa. Viceversa, $FP$ e $FN$ rappresentano i `falsi positivi' ed i `falsi negativi': esempi rispettivamente con etichetta negativa e positiva.

    Seppur sia una delle metriche più comuni, l'accuratezza può spesso fornire una visione della bontà del modello molto diversa da quella reale: per mostrare perché questa affermazione sia vera è utile introdurre un esempio di classificatore con la matrice di confusione\footnote{Matrice che pone sulle righe le etichette di un oggetto del testing set e sulle colonne le predizioni effettuate dal classificatore: l'elemento di posto $(i, j)$ di questa matrice quantifica il numero di volte in cui il classificatore ha predetto per un oggetto con etichetta $i$ il valore $j$.} riportata in Tabella \ref{tab:matriceConfusione}.

    \begin{table}[H]
        \centering
        \begin{tabular}{l|ll}
                            & \textbf{Positivo} & \textbf{Negativo} \\ \hline
        \textbf{Positivo} & 0                 & 10                 \\
        \textbf{Negativo} & 0                 & 1000             
        \end{tabular}
        \caption{Esempio di matrice di confusione. Sulle righe vengono indicate le etichette, sulle colonne le predizioni.}  
        \label{tab:matriceConfusione}    
    \end{table}

    Il classificatore in questo caso etichetta ogni esempio come negativo, di conseguenza non è sicuramente considerabile buono. Calcolando l'accuratezza otteniamo però un valore pari a $1000/1010 \approx 0.99$, che invece sembra indicare un ottimo classificatore. Questa debolezza della metrica risulta più evidente in casi in cui, come in quello dell'esempio (o del nostro dataset, seppur in scala minore), il dataset presenta un forte sbilanciamento tra le etichette.

    \paragraph{Precisione}
    Intuitivamente, questa metrica quantifica, appunto, la precisione del modello nel predire una determinata etichetta. Ad esempio, avere una precisione alta per la classe dei positivi indica che il modello molto probabilmente non sbaglierà quando etichetta un elemento come positivo.

    Nel contesto della classificazione binaria questa metrica viene calcolata in modo differente per le due etichette presenti. Assumendo di essere nella classe dei positivi, la precisione è definita come: 
    \begin{equation}
        \mathrm{Precisione} = \frac{VP}{VP + FP}.
    \end{equation}


    \paragraph{Recupero} Intuitivamente, questa metrica quantifica la capacità del modello di riconoscere una determinata etichetta. Ad esempio, un recupero basso per i positivi implica un elevato numero di falsi negativi.
    
    Prendendo in considerazione, anche in questo caso, la classe dei positivi, il recupero è definito come:
    \begin{equation}
        \mathrm{Recupero} = \frac{VP}{VP + FN}.      
    \end{equation}

    \paragraph{F1-Score}
    Metrica che combina precisione e recupero tramite la media armonica di queste due quantità, fornendo quindi un valore che contiene informazioni in entrambi i sensi. Formalmente, può essere espressa come: 
    \begin{equation}
        \text{F1-score} = 2\frac{\mathrm{precisione} \cdot \mathrm{recupero}}{\mathrm{precisione} + \mathrm{recupero}}.
    \end{equation}
    Si noti che se una tra precisione e recupero equivale a 0, anche l'F1-score sarà 0; al contrario, l'F1-score può assumere valore 1 solo se entrambe le metriche hanno valore 1.


\end{document}