\documentclass[../../main.tex]{subfiles}

\graphicspath{{\subfix{../../immagini/}}}

\begin{document}
    \paragraph{Filtri appresi}
    L'implementazione del filtro di Bloom e delle due varianti apprese riprende quella fornita in \cite{ma2020}: Il codice utilizzato è pressoché equivalente, fatte salve alcune modifiche utili a permettere l'utilizzo dei filtri appresi anche con il percettrone.

    \paragraph{Suddivisione del dataset}
    Per la parte di esperimenti dedicata al confronto delle performance dei classificatori la suddivisione viene fatta coerentemente con la tecnica utilizzata: nel caso della convalida incrociata, ad esempio, il dataset viene suddiviso in $k$ sottoinsiemi, come descritto nel Paragrafo \ref{sec:metrichevalutazione}.

    Di contro, nella seconda parte di esperimenti l'impostazione del dataset riprende quella presentata in \cite{ma2020}.

    \paragraph{Codifiche degli elementi del dataset}
    Gli URL vengono codificati in modo differente nelle due tipologie di classificatori.

    Per la rete ricorrente, come descritto in \cite{ma2020}, gli URL vengono codificati assegnando ad ogni carattere un numero intero tra 1 e 128 in ordine di frequenza, se, ad esempio, il carattere `e' è quello più frequente verrà mappato al numero 1, i caratteri in posizione successiva alla 128 vengono tutti mappati a 0. Infine, se un URL è più corto di 150 caratteri viene applicato un padding di 0, viceversa, se l'URL è più lungo di 150 caratteri verrà troncato in tale posizione.

    Per il percettrone multistrato, invece, gli URL vengono codificati sfruttando la classe \texttt{CountVectorizer} della libreria Scikit-Learn, ogni URL viene quindi mappato ad un vettore contenente le frequenze assolute di ognuno dei caratteri presenti nel training set.

    La scelta di utilizzare due codifiche differenti deriva da due principali motivi: in primo luogo, gli esperimenti iniziali mostravano che l'utilizzo della prima codifica anche per il percettrone portava ad avere reti troppo grandi per le prestazioni fornite, e di conseguenza a filtri meno efficienti rispetto a quelli del sopracitato articolo.\\
    In secondo luogo, l'alternativa di utilizzare questa seconda codifica anche per la rete ricorrente non era praticabile, infatti nella suddetta rete è presente anche uno strato di embedding, che lavora associando ad ognuno dei caratteri un vettore di 5 elementi. Utilizzando la seconda codifica ognuno degli elementi del vettore non avrebbe più rappresentato un carattere, ma una frequenza, cambiando di fatto le informazioni fornite dallo strato di embedding. Viceversa, eliminare lo strato di embedding per adattare la rete alla nuova codifica avrebbe rappresentato una contraddizione rispetto agli scopi degli esperimenti, che sono invece quelli di confrontare nuovi classificatori con la stessa RNN dell'articolo.

    È importante notare come la seconda codifica perda le informazioni legate alla posizione dei caratteri nell'URL, come evidenziato dai risultati, però, l'assenza di queste informazioni non comporta grosse perdite nelle performance dei percettroni.

    Una terza codifica per i percettroni viene inoltre introdotta solamente nella seconda parte di esperimenti: ogni carattere viene mappato con gli stessi criteri della prima codifica, ma l'URL codificato sarà composto dalla concatenazione delle rappresentazioni binarie di ognuno degli interi associati ai caratteri. Il numero di bit utilizzato per ogni intero è il minimo per permettere una rappresentazione di ognuno dei caratteri. Anche in questo caso ad ogni URL verrà applicato un padding o un troncamento, per avere vettori della stessa dimensione.

    In ultimo, è importante sottolineare che, per evitare un eventuale introduzione di bias, nel caso della prima e terza codifica i dizionari che vengono utilizzati per il mapping dei caratteri vengono calcolati solamente sull'insieme d'addestramento. In realtà, calcolare i dizionari su tutto il dataset non dovrebbe portare all'introduzione di informazioni aggiuntive, le codifiche infatti semplicemente mappano i caratteri di ogni URL in interi, in questo caso scelti a seconda delle frequenze dei caratteri scelti.


    \paragraph{Classificatori}
    Come già accennato, il codice utilizzato per la GRU è equivalente a quello fornito in \cite{ma2020}, l'implementazione della rete ricorrente quindi è fatta sfruttando la libreria PyTorch. Il percettrone, invece, viene implementato sfruttando la libreria Keras\footnote{Libreria Python per l'apprendimento automatico. Fornisce un'interfaccia ad alto livello di altre librerie simili, più a basso livello.}.

    \paragraph{Serializzazione dei modelli}
    Dato che molti degli esperimenti comprendono lo spazio come unità di misura delle prestazioni di un filtro appreso, è utile ragionare sulla serializzazione dei modelli utilizzati, per permettere un confronto che non sia influenzato da possibili compressioni effettuate dalle funzioni di libreria.

    Per assicuraci di avere dei modelli confrontabili, la soluzione è utilizzare Pickle\footnote{Libreria che implementa procedure utili alla serializzazione e de-serializzazione di oggetti Python. Permette quindi di convertire oggetti in sequenze di byte, in modo da poterli salvare su database o disco \cite{Pickle}.} per serializzare entrambi i modelli (e di conseguenza anche per ricaricarli). Nello specifico, verranno salvati solamente i parametri di ognuno dei modelli, mentre la loro architettura verrà ricreata ad ogni caricamento.
    
\end{document}